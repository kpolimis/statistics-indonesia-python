{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1474431676.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to kpolimis@u.washington.edu and will expire on July 29, 2017.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import graphlab as gl\n",
    "from gensim.models import word2vec\n",
    "from utils import get_Sukarno_Bandung_speech, convert_pdf_to_txt, speech_to_wordlist, speech_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get President Sukarno's speeches\n",
    "* 18th April 1955  \n",
    "  Bandung, Indonesia\n",
    " \n",
    " \n",
    "* 19th December 1961  \n",
    "  Jogjakarta, Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sukarno_bandung_speech.pdf', 'already exists')\n"
     ]
    }
   ],
   "source": [
    "get_Sukarno_Bandung_speech()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert the .pdf of speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sukarno_bandung_speech = convert_pdf_to_txt('sukarno_bandung_speech.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subset the speech to remove citation material in document heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your Excellencies,Ladies and Gentlemen, Sisters and Brothers.It is my great honour and privilege on this historic day to bid you welcome to Indonesia. On behalf of the people and government of Indonesia - your hosts - I beg your understanding and forbearance if some circumstances in our country do not meet your expectation. We have, I assure you, done our best to make your stay amongst us memorable for both our guests and your hosts. We hope that the warmth of our welcome will compensate for whatever material shortcomings there may be.As I survey this hall and the distinguished guests gathered here, my heart is filled with emotion. This is the first intercontinental conference of coloured peoples in the history of mankind! I am proud that my country is your host. I am happy that you were able to accept the invitations'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sukarno_bandung_speech = sukarno_bandung_speech[1167:len(sukarno_bandung_speech)]\n",
    "sukarno_bandung_speech[0:829]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vector models\n",
    "* [Word2vec](https://code.google.com/p/word2vec/), published by Google in 2013, is a neural network implementation that learns [distributed representations](http://www.cs.toronto.edu/~bonner/courses/2014s/csc321/lectures/lec5.pdf) for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "* clean the speech and convert to a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'excellencies', 'ladies', 'and', 'gentlemen', 'sisters', 'and', 'brothers', 'it', 'is']\n"
     ]
    }
   ],
   "source": [
    "sukarno_bandung_speech_wordlist = speech_to_wordlist(sukarno_bandung_speech)\n",
    "print sukarno_bandung_speech_wordlist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from bandung speech\n"
     ]
    }
   ],
   "source": [
    "sukarno_bandung_speech_sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from bandung speech\"\n",
    "sukarno_bandung_speech_sentences += speech_to_sentences(sukarno_bandung_speech, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'excellencies', 'ladies', 'and', 'gentlemen', 'sisters', 'and', 'brothers', 'it', 'is', 'my', 'great', 'honour', 'and', 'privilege', 'on', 'this', 'historic', 'day', 'to', 'bid', 'you', 'welcome', 'to', 'indonesia']\n"
     ]
    }
   ],
   "source": [
    "print sukarno_bandung_speech_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences in bandung speech: 204\n"
     ]
    }
   ],
   "source": [
    "print \"sentences in bandung speech: %d\" % len(sukarno_bandung_speech_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sukarno_bandung_speech_sentences, workers=num_workers, \n",
    "            size=num_features, min_count = min_word_count, \n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "os.chdir('../text_analysis/data/')\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=83, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'all', 'own', 'give', 'is', 'africa', 'life', 'one', 'as', 'been']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab.keys()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('each', 0.9954915642738342),\n",
       " ('africa', 0.9954866170883179),\n",
       " ('the', 0.9954788684844971),\n",
       " ('give', 0.9954684972763062),\n",
       " ('world', 0.9954482316970825),\n",
       " ('let', 0.9954192638397217),\n",
       " ('and', 0.9954022169113159),\n",
       " ('of', 0.9953722953796387),\n",
       " ('that', 0.9953322410583496),\n",
       " ('other', 0.9953094720840454)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"colonialism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
